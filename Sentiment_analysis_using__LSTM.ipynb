{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# *Naan Mudhalvan  Project*\n",
        "# *Generative AI*\n",
        "# *Project Title - Sentimental Analysis Using LSTM*\n",
        "#*Register number-211521243099*\n",
        "# *Name - I. Mohamed Sameer*"
      ],
      "metadata": {
        "id": "tGCLFUPQi_KH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-8j7LaYFq4G",
        "outputId": "7db28882-2134-48ff-8b51-e8e2afaedbe3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting Keras-Preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from Keras-Preprocessing) (1.25.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from Keras-Preprocessing) (1.16.0)\n",
            "Installing collected packages: Keras-Preprocessing\n",
            "Successfully installed Keras-Preprocessing-1.1.2\n"
          ]
        }
      ],
      "source": [
        "pip install Keras-Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucSrAiJQFu6P",
        "outputId": "6e2ee87d-37d5-4ae3-fb98-934aeebabf73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkLFYO7iFeN-",
        "outputId": "9530d469-bec4-457c-96c4-55fd0a8cd644"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import accuracy_score\n",
        "import math\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data\n",
        "data = pd.read_csv('/content/drive/MyDrive/generative ai naan mudhalvan/IMDB Dataset.csv')\n",
        "# i have added the dataset in my repository use it since i use google colab i mounted my dataset in my drive"
      ],
      "metadata": {
        "id": "Z1YtteVgUroT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to remove tags from text\n",
        "def remove_tags(string):\n",
        "    removelist = \"\"\n",
        "    result = re.sub('<.*?>', '', string)  # Remove HTML tags\n",
        "    result = re.sub('https://.*', '', result)  # Remove URLs\n",
        "    result = re.sub(r'[^w'+removelist+']', ' ', result)  # Remove non-alphanumeric characters\n",
        "    result = result.lower()\n",
        "    return result"
      ],
      "metadata": {
        "id": "JZQU0PNUUwtj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing to 'review' column\n",
        "data['review'] = data['review'].apply(lambda cw : remove_tags(cw))\n"
      ],
      "metadata": {
        "id": "doJSdyaCU7xr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Further preprocessing: remove stopwords and lemmatize\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "data['review'] = data['review'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
        "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "def lemmatize_text(text):\n",
        "    st = \"\"\n",
        "    for w in w_tokenizer.tokenize(text):\n",
        "        st = st + lemmatizer.lemmatize(w) + \" \"\n",
        "    return st\n",
        "data['review'] = data.review.apply(lemmatize_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIOBd_B5VDU-",
        "outputId": "8acafb67-0cd2-42f6-e481-4e2f9b00ec90"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate average length of each review\n",
        "s = sum(len(review.split()) for review in data['review'])\n",
        "average_length = s / len(data['review'])\n",
        "print(\"Average length of each review:\", average_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQsZUx4ZVIBa",
        "outputId": "f090ba73-082b-452b-b1c8-27143a2d47cf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average length of each review: 18.7137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Percentage of positive and negative reviews\n",
        "pos_percentage = (data['sentiment'].value_counts(normalize=True)['positive']) * 100\n",
        "neg_percentage = (data['sentiment'].value_counts(normalize=True)['negative']) * 100\n",
        "print(\"Percentage of reviews with positive sentiment:\", pos_percentage, \"%\")\n",
        "print(\"Percentage of reviews with negative sentiment:\", neg_percentage, \"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZx7yLNyVLLa",
        "outputId": "342ce0a0-8ceb-4643-f546-5b6502b616a7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of reviews with positive sentiment: 50.0 %\n",
            "Percentage of reviews with negative sentiment: 50.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for training\n",
        "reviews = data['review'].values\n",
        "labels = data['sentiment'].values\n",
        "encoder = LabelEncoder()\n",
        "encoded_labels = encoder.fit_transform(labels)\n",
        "train_sentences, test_sentences, train_labels, test_labels = train_test_split(reviews, encoded_labels, stratify=encoded_labels)"
      ],
      "metadata": {
        "id": "8i8uViYzVNzo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "vocab_size = 3000\n",
        "oov_tok = ''\n",
        "embedding_dim = 100\n",
        "max_length = 200\n",
        "padding_type = 'post'\n",
        "trunc_type = 'post'"
      ],
      "metadata": {
        "id": "QJcl5x36VUkt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize sentences\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "word_index = tokenizer.word_index\n"
      ],
      "metadata": {
        "id": "wMoYMFPXVV5E"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert train dataset to sequence and pad sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "train_padded = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length)\n"
      ],
      "metadata": {
        "id": "IJyyukSLVYXq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert test dataset to sequence and pad sequences\n",
        "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "test_padded = pad_sequences(test_sequences, padding=padding_type, maxlen=max_length)\n"
      ],
      "metadata": {
        "id": "Oq_u0wbQVaz6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model definition\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    keras.layers.Bidirectional(keras.layers.LSTM(64)),\n",
        "    keras.layers.Dense(24, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "PaO0QaU3Ve_s"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XQ12ZHDVjYR",
        "outputId": "22c6322a-813e-4072-da3f-9b998322232a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 200, 100)          300000    \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirecti  (None, 128)               84480     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 24)                3096      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 25        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 387601 (1.48 MB)\n",
            "Trainable params: 387601 (1.48 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model training\n",
        "num_epochs = 5\n",
        "history = model.fit(train_padded, train_labels, epochs=num_epochs, verbose=1, validation_split=0.1)\n",
        "# since epoch takes high time\n",
        "#add higher number of epoch for better  prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb90JD5iVmEJ",
        "outputId": "7d510395-a758-452d-fb7a-5d99fd5b1c2c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1055/1055 [==============================] - 253s 235ms/step - loss: 0.6931 - accuracy: 0.5007 - val_loss: 0.6926 - val_accuracy: 0.5189\n",
            "Epoch 2/5\n",
            "1055/1055 [==============================] - 246s 233ms/step - loss: 0.6928 - accuracy: 0.5101 - val_loss: 0.6921 - val_accuracy: 0.5272\n",
            "Epoch 3/5\n",
            "1055/1055 [==============================] - 281s 266ms/step - loss: 0.6926 - accuracy: 0.5129 - val_loss: 0.6939 - val_accuracy: 0.5133\n",
            "Epoch 4/5\n",
            "1055/1055 [==============================] - 241s 229ms/step - loss: 0.6925 - accuracy: 0.5153 - val_loss: 0.6921 - val_accuracy: 0.5163\n",
            "Epoch 5/5\n",
            "1055/1055 [==============================] - 242s 229ms/step - loss: 0.6924 - accuracy: 0.5157 - val_loss: 0.6925 - val_accuracy: 0.5144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model evaluation\n",
        "test_loss, test_accuracy = model.evaluate(test_padded, test_labels)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dv45V21cVnRL",
        "outputId": "8467d31e-b383-4181-9271-16becbc243b8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "391/391 [==============================] - 21s 55ms/step - loss: 0.6926 - accuracy: 0.5117\n",
            "Test Accuracy: 0.511680006980896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment prediction for custom sentences\n",
        "# In the below sentence list  you can give your own sentence also to check the model performance\n",
        "sentence = [\"The movie was very touching and heartwarming\",\n",
        "            \"I have never seen a terrible movie like this\",\n",
        "            \"The movie plot is terrible but it had good acting\",\n",
        "            \"waste of time for watching\"]\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentence)\n",
        "padded = pad_sequences(sequences, padding=padding_type, maxlen=max_length)\n",
        "\n",
        "predictions = model.predict(padded)\n",
        "\n",
        "for i in range(len(sentence)):\n",
        "    print(sentence[i])\n",
        "    if predictions[i] >= 0.5:\n",
        "        print(\"Predicted sentiment: Positive\")\n",
        "    else:\n",
        "        print(\"Predicted sentiment: Negative\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-c7csYUVpfo",
        "outputId": "846d76ba-cebf-438a-ec3a-343fa009e599"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 40ms/step\n",
            "The movie was very touching and heartwarming\n",
            "Predicted sentiment: Positive\n",
            "I have never seen a terrible movie like this\n",
            "Predicted sentiment: Positive\n",
            "The movie plot is terrible but it had good acting\n",
            "Predicted sentiment: Positive\n",
            "waste of time for watching\n",
            "Predicted sentiment: Positive\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}